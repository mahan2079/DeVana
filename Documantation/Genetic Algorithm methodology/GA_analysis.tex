\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{graphicx}
\usepackage{float}
\usepackage{geometry}
\usepackage{tikz}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{subcaption}
\usepackage{enumitem}
\usepackage{setspace}
\usepackage{stackengine}

% Glossaries and acronyms
\usepackage[acronym]{glossaries}
\makeglossaries

% Project metadata and convenience macros
\newcommand{\softwareName}{DeVana}
\newcommand{\softwareVersion}{v0.4.1}

% Hyperref setup
\hypersetup{
    pdftitle={Advanced Genetic Algorithm Optimization for Dynamic Vibration Absorber Design: A Comprehensive Methodology},
    pdfauthor={Master's Thesis Methodology},
    pdfsubject={\softwareName{} \softwareVersion{} open-source thesis},
    colorlinks=true,
    linkcolor=blue,
    citecolor=blue,
    urlcolor=blue
}

% Acronyms
\newacronym{dva}{DVA}{Dynamic Vibration Absorber}
\newacronym{DVA}{DVA}{Dynamic Vibration Absorber} % for existing \gls{DVA} usage
\newacronym{frf}{FRF}{Frequency Response Function}
\newacronym{ga}{GA}{Genetic Algorithm}
\newacronym{pso}{PSO}{Particle Swarm Optimization}
\newacronym{sa}{SA}{Simulated Annealing}
\newacronym{de}{DE}{Differential Evolution}
\newacronym{cmaes}{CMA-ES}{Covariance Matrix Adaptation Evolution Strategy}
\newacronym{rl}{RL}{Reinforcement Learning}
\newacronym{knn}{KNN}{k-Nearest Neighbors}
\newacronym{hdi}{HDI}{Highest Density Interval}
\newacronym{ucb}{UCB}{Upper Confidence Bound}
\newacronym{ei}{EI}{Expected Improvement}
\newacronym{gui}{GUI}{Graphical User Interface}
\newacronym{kpi}{KPI}{Key Performance Indicator}
\newacronym{qmc}{QMC}{Quasi-Monte Carlo}

% Page setup
\geometry{margin=2.5cm}
\onehalfspacing

% Code listing setup
\lstset{
    basicstyle=\ttfamily\footnotesize,
    breaklines=true,
    frame=single,
    numbers=left,
    numberstyle=\tiny,
    keywordstyle=\color{blue},
    commentstyle=\color{green!60!black},
    stringstyle=\color{red},
    backgroundcolor=\color{gray!10}
}

% Title and author
\title{Advanced Genetic Algorithm Optimization for Dynamic Vibration Absorber Design: A Comprehensive Methodology}
\author{Master's Thesis Methodology}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
\softwareName{} (\softwareVersion{}) is an open-source, first-of-its-kind playground for the design of \glspl{dva} that unifies classical and modern optimization under one reproducible, extensible framework. The platform integrates a rigorous mechanical model (fully coupled 2DOF--3DOF), an \gls{frf}-based fitness pipeline, and a suite of optimizers (\gls{ga}, \gls{pso}, \gls{sa}, \gls{cmaes}, \gls{rl}, and \gls{de}). Beyond providing many algorithms, \softwareName{} contributes three methodological advances tailored to vibration absorber design: (i) adaptive operator control for \gls{ga} via ML-bandit and \gls{rl} controllers that tune crossover, mutation, and population size online; (ii) surrogate-assisted screening using a \gls{knn} predictor to reduce expensive \gls{frf} evaluations; and (iii) advanced population seeding (random, Sobol, Latin hypercube, and a NeuralSeeder with \gls{ucb}/\gls{ei} acquisition) for improved exploration. The system instruments each run with detailed metrics and provides statistical post-analysis across repeated runs to recommend robust parameter ranges for every DVA variable. We document the mathematical formulation, software architecture, algorithmic controllers, benchmarking protocol, and statistical synthesis pipeline, and we release the full implementation to accelerate research and practice in vibration control.
\end{abstract}

\noindent\textbf{Keywords:} Dynamic vibration absorber, frequency response function, genetic algorithm, CMA-ES, PSO, simulated annealing, differential evolution, reinforcement learning, surrogate modeling, seeding, robustness, reproducibility, open-source.

\vspace{0.5em}
\noindent\textbf{Software:} \softwareName{} \softwareVersion{} (open-source; available on GitHub and project website).

\tableofcontents
\newpage

% List of acronyms
\printglossary[type=\acronymtype,title=List of Acronyms]
\newpage

\section{Introduction and Motivation}

This thesis presents \softwareName{}, a comprehensive optimization framework and software platform for designing \glspl{dva} with practical applicability in mechanical engineering. The framework addresses the challenges of vast configuration spaces, multi-criteria optimization, and computational efficiency by (i) supporting multiple optimizers within a single configurable playground, (ii) introducing adaptive controllers for \gls{ga} that learn online, (iii) reducing evaluation cost via surrogate-assisted screening, and (iv) enabling statistical synthesis of recommended parameter ranges across repeated runs.

\subsection{Contributions and novelties}
\label{subsec:contributions}
\begin{enumerate}
    \item \textbf{Unified optimization playground:} A modular, open-source platform that allows designers to configure and compare \gls{ga}, \gls{pso}, \gls{sa}, \gls{cmaes}, \gls{rl}, and \gls{de} on identical problems with consistent instrumentation.
    \item \textbf{Adaptive \gls{ga} controllers:} Online tuning of crossover, mutation, and population size via an ML-bandit controller and a compact \gls{rl} agent, overcoming fixed-operator limitations.
    \item \textbf{Surrogate-assisted screening:} A \gls{knn}-based pre-filter that reduces the number of costly \gls{frf} evaluations while preserving exploration through novelty.
    \item \textbf{Advanced seeding:} Random, Sobol, and Latin hypercube \gls{qmc} options plus a NeuralSeeder that proposes individuals using \gls{ucb}/\gls{ei}, improving early search quality.
    \item \textbf{Statistical range synthesis:} A robust pipeline that aggregates multiple independent runs and outputs recommended parameter ranges per DVA variable using complementary criteria (IQR, P5--P95, Tukey, \gls{hdi}, Top-$q$, TMAD).
    \item \textbf{Reproducible engineering workflow:} Versioned release (\softwareVersion{}), open dataset of metrics, and exportable configurations for repeatable studies and fair comparisons.
\end{enumerate}

\subsection{Software availability and versioning}
\label{subsec:availability}
\softwareName{} is released under an open-source license at GitHub and the project website. This thesis documents \softwareVersion{}. The repository includes source code, example configurations, and instructions to reproduce all experiments presented herein.

\subsection{Thesis roadmap}
\label{subsec:roadmap}
We begin with the DVA configuration and parameter spaces, followed by the mechanical model and its \gls{frf}-based evaluation. We then detail the optimization problem, the traditional \gls{ga} baseline, and the advanced features. Subsequent sections describe software architecture, the algorithm suite and configuration playground, benchmarking and robustness analysis, statistical range synthesis, and the overall results, before concluding with reproducibility notes and future work.


\section{Software architecture and optimization playground}
\label{sec:architecture}

\subsection{Design principles}
\begin{itemize}
    \item \textbf{Modularity}: separable layers for mechanics, optimization, controllers, seeding, surrogate, \gls{gui}, and benchmarking.
    \item \textbf{Reproducibility}: versioned configurations, deterministic seeds on demand, and exportable results.
    \item \textbf{Extensibility}: clean interfaces to add new optimizers, surrogates, and metrics with minimal code changes.
    \item \textbf{Observability}: rich metrics, logs, and artifacts captured per run and per generation.
\end{itemize}

\subsection{System overview}
\begin{enumerate}
    \item \textbf{Mechanical core}: builds the mass, damping, stiffness, and force operators and computes \gls{frf}s per Sec.~\ref{subsubsec:chosen_performance_criteria_combined}.
    \item \textbf{Fitness pipeline}: evaluates Eq.~\eqref{Eq.objective_function_detailed} using modal, inter-modal, and global criteria with user-defined weights.
    \item \textbf{Algorithm suite}: \gls{ga}, \gls{pso}, \gls{sa}, \gls{cmaes}, \gls{rl}, and \gls{de}, with consistent bounds, fixed-parameter handling, and termination.
    \item \textbf{Controllers}: ML-bandit and \gls{rl} for adaptive \gls{ga} operator control (Sec.~\ref{Eq.ucb}--\ref{Eq.bandit_reward}).
    \item \textbf{Seeding}: Random, Sobol, LHS, and NeuralSeeder (\gls{ucb}/\gls{ei}) as in Sec.~\ref{Eq.harmonic.solution.2dof3dof} onward.
    \item \textbf{Surrogate screening}: \gls{knn}-based pre-filter with novelty (Sec.~\ref{sec:benchmarking}).
    \item \textbf{Instrumentation}: per-generation and system \gls{kpi}s (Sec.~\ref{sec:benchmarking}).
    \item \textbf{GUI}: interactive configuration, execution control, visualization (responses, convergence, ranges), and export.
\end{enumerate}

\subsection{Optimization playground}
Users configure experiments by selecting the optimizer, bounds and fixed masks, objective weights, seeding, controllers, surrogate settings, and stopping criteria. The same mechanical problem can thus be solved by multiple methods, enabling apples-to-apples comparisons with common metrics and visualizations.

\subsection{Data, logging, and metrics}
At each generation we record time, evaluations, fitness statistics, operator rates, population size, and resource usage, along with artifacts (best solutions, FRFs, convergence traces). Artifacts are exportable as CSV/JSON for downstream analysis.

\subsection{Extensibility}
New optimizers, surrogates, or controllers implement small interfaces (ask, tell/evaluate, and adapt hooks). New performance metrics can be registered and automatically included in the logging and reporting layers.

\subsection{Reproducibility}
All experiments store configuration hashes and software version (\softwareVersion{}), aiding peer reproduction. We provide example configurations used for this thesis alongside raw metric exports.


\section{Configuration Space of DVA Systems}

The design of \glspl{DVA} involves selecting appropriate combinations of mechanical components—masses, springs, dampers, and inerters—to attach to a primary system for vibration mitigation. Each component contributes to the system's dynamic behavior, and their combinations result in a multitude of possible configurations.

\subsection{Definition of Components and Parameters}

Let:

\begin{itemize}
    \item $\mathcal{M} = \{ m_i \mid m_i \in [m_i^{\min}, m_i^{\max}],\ i=1,\ldots,n_m \}$: Set of mass elements with viable ranges.
    \item $\mathcal{K} = \{ k_j \mid k_j \in [k_j^{\min}, k_j^{\max}],\ j=1,\ldots,n_k \}$: Set of spring elements with viable stiffness ranges.
    \item $\mathcal{C} = \{ c_l \mid c_l \in [c_l^{\min}, c_l^{\max}],\ l=1,\ldots,n_c \}$: Set of damping elements with viable damping coefficient ranges.
    \item $\mathcal{B} = \{ b_p \mid b_p \in [b_p^{\min}, b_p^{\max}],\ p=1,\ldots,n_b \}$: Set of inerter elements with viable inertance ranges.
\end{itemize}

Each parameter is bounded within a feasible design range, reflecting practical engineering constraints such as material properties, geometric limitations, and manufacturing capabilities.

\subsection{Total Number of Configurations}



\begin{align}
N_{\text{total}} = \left( n_m + n_k + n_c + n_b \right)\\
N_{\text{config}} = \frac{N_{\text{total}} \times (N_{\text{total}} + 1)}{2}
\label{eq:N_config}
\end{align}

This growth underscores the impracticality of exhaustively evaluating every possible configuration due to computational limitations.


\section{Parameter Space and Design Variables}

For a given configuration $s$, the parameter vector $\boldsymbol{\theta}_s$ comprises the design variables associated with the included components:

\begin{equation}
\boldsymbol{\theta}_s = [ \theta_1, \theta_2, \ldots, \theta_{n_s} ]^\top
\label{eq:theta_s}
\end{equation}

where $n_s$ is the number of parameters in configuration $s$, and each $\theta_i$ corresponds to a component parameter (e.g., mass, stiffness, damping coefficient, or inertance) within its viable range:

\begin{equation}
\theta_i \in [ \theta_i^{\min}, \theta_i^{\max} ]
\label{eq:theta_i_range}
\end{equation}

The feasible parameter space for configuration $s$ is thus defined as:

\begin{equation}
\Theta_s = \prod_{i=1}^{n_s} [ \theta_i^{\min}, \theta_i^{\max} ]
\label{eq:Theta_s}
\end{equation}




\section{Problem Statement}

\subsection{Mechanical System Definition}

\subsubsection{Fully Coupled 2DOF - 3DOF System Setup}
\paragraph{System Overview and Configuration}

In order to comprehend the fundamental concepts and operational principles of the advanced genetic algorithm methodology introduced in this work, a comprehensive, step-by-step analysis of the mechanical system is essential. The system under investigation is a sophisticated fully coupled 2DOF - 3DOF system, where "fully coupled" signifies that all system components are interconnected through masses, springs, dampers, and inerters, creating a complete vibrational system with comprehensive dynamic interactions.

The primary structure consists of a 2DOF main system with two primary masses representing different structural elements, augmented by three strategically positioned 1DOF Dynamic Vibration Absorbers (DVAs). This configuration allows for multi-modal vibration control targeting multiple resonant frequencies simultaneously. The system architecture includes:

\begin{itemize}
    \item \textbf{Primary structural elements}: Two primary masses ($M_1$, $M_2$) representing the main structural components with their inertial properties
    \item \textbf{Dynamic Vibration Absorbers}: Three DVA masses ($\mu_1$, $\mu_2$, $\mu_3$) with independent dynamic characteristics
    \item \textbf{Base excitations}: Lower and upper base motions providing external vibrational inputs
    \item \textbf{External forcing}: Direct force inputs applied to the primary masses
    \item \textbf{Complete coupling}: All components interconnected through mass, stiffness, damping, and inertial elements
\end{itemize}

The system's complexity arises from the extensive parameter space and coupling mechanisms. The complete system comprises 48 independent design parameters distributed across:
\begin{itemize}
    \item 15 mass coupling parameters ($\beta_1$ through $\beta_{15}$) for inertial interconnections
    \item 15 stiffness parameters ($\lambda_1$ through $\lambda_{15}$) for elastic coupling
    \item 15 damping parameters ($\nu_1$ through $\nu_{15}$) for energy dissipation
    \item 3 DVA mass parameters ($\mu_1$, $\mu_2$, $\mu_3$) for absorber sizing
\end{itemize}

This high-dimensional parameter space necessitates advanced optimization techniques capable of efficiently exploring the design domain while maintaining computational tractability.

\paragraph{Vibrational Modeling of the System and Assumptions}

The main 2DOF system is modeled using a comprehensive framework comprising masses, springs, dampers, and inerters, interconnected with both upper and lower bases. The modeling approach incorporates the following fundamental assumptions:

\begin{itemize}
    \item \textbf{Linear elastic behavior}: All stiffness elements ($K_1$, $K_2$, $K_3$) exhibit linear force-displacement relationships within the operational range
    \item \textbf{Linear viscous damping}: All damping elements ($C_1$, $C_2$, $C_3$) follow linear velocity-dependent force relationships
    \item \textbf{Point mass representation}: All masses are treated as point masses with concentrated inertial properties
    \item \textbf{One-dimensional motion}: All system components move in a single translational direction
    \item \textbf{Time-invariant parameters}: All system parameters remain constant during operation
    \item \textbf{No geometric nonlinearities}: The system operates within the linear regime, excluding geometric stiffening effects
    \item \textbf{Perfect bonding}: All interconnections between components are assumed to be rigid and perfect
\end{itemize}

The base excitations are modeled with flexibility to represent various mechanical scenarios:
\begin{itemize}
    \item \textbf{Foundation motion}: Representing actual physical ground motion or support excitation
    \item \textbf{Additional system modes}: Modeling extra degrees of freedom from more complex structures
    \item \textbf{Boundary condition variations}: Accommodating different support and mounting conditions
\end{itemize}

Each DVA is designed as an independent 1DOF system with its own mass, stiffness, damping, and inertial elements. The coupling between the primary system and DVAs, as well as between DVAs themselves, is achieved through comprehensive interconnection elements ensuring complete dynamic coupling.

\paragraph{Derivation of Governing Equations}

The governing equations for the fully coupled 2DOF-3DOF system are derived using Newton's method, applying Newton's second law to each degree of freedom while accounting for all interconnecting forces. The system's generalized coordinates are defined as:

\begin{align}\label{Eq.generalized.coordinate.2dof3dof}
    \mathbf{q} =
    \begin{bmatrix}
        U_1(t) \\
        U_2(t) \\
        u_1(t) \\
        u_2(t) \\
        u_3(t)
    \end{bmatrix}; \quad
    \dot{\mathbf{q}} =
    \begin{bmatrix}
        \dot{U_1}(t) \\
        \dot{U_2}(t) \\
        \dot{u_1}(t) \\
        \dot{u_2}(t) \\
        \dot{u_3}(t)
    \end{bmatrix}; \quad
    \ddot{\mathbf{q}} =
    \begin{bmatrix}
        \ddot{U_1}(t) \\
        \ddot{U_2}(t) \\
        \ddot{u_1}(t) \\
        \ddot{u_2}(t) \\
        \ddot{u_3}(t)
    \end{bmatrix}
\end{align}

where:
\begin{itemize}
    \item $U_1(t)$, $U_2(t)$: Displacements of the primary masses at time $t$
    \item $u_1(t)$, $u_2(t)$, $u_3(t)$: Displacements of the DVA masses at time $t$
\end{itemize}

The equations of motion are expressed in matrix form as:

\begin{equation} \label{Eq.EOM_dimensional_combined}
    \mathbf{M} \ddot{\mathbf{q}} + \mathbf{C} \dot{\mathbf{q}} + \mathbf{K} \mathbf{q} = \mathbf{F}(t)
\end{equation}

where:

\begin{itemize}
    \item $\mathbf{q}$: Generalized displacement vector, capturing displacements of the primary and \gls{DVA} masses.
    \item $\mathbf{M}$: Mass matrix.
    \item $\mathbf{C}$: Damping matrix.
    \item $\mathbf{K}$: Stiffness matrix.
    \item $\mathbf{F}(t)$: External force vector, which includes both external loads and base motion effects.
\end{itemize}

The generalized coordinate vector is defined as:

\begin{equation}\label{Eq.generalized_coordinate_combined}
    \mathbf{q} = 
  \begin{bmatrix}
    U_1 \\ U_2 \\ u_1 \\ u_2 \\ u_3
  \end{bmatrix}
\end{equation}

\paragraph{Mass Matrix}

\begin{equation}\label{Eq.mass_matrix_dimensional_combined}
\begin{aligned}
[M] =& 
\begin{bmatrix}
\shortstack[c]{$M_1 + b_1$ \\ $+ b_2 + b_3$} & 0 & \shortstack[c]{$-b_1$ \\ \,} & \shortstack[c]{$-b_2$ \\ \,} & \shortstack[c]{$-b_3$ \\ \,} \\
0 & \shortstack[c]{$M_2 + b_4$ \\ $+ b_5 + b_6$} & \shortstack[c]{$-b_4$ \\ \,} & \shortstack[c]{$-b_5$ \\ \,} & \shortstack[c]{$-b_6$ \\ \,} \\
\shortstack[c]{$-b_1$ \\ \,} & \shortstack[c]{$-b_4$ \\ \,} & \shortstack[c]{$m_1 + b_1$ \\ $+ b_4 + b_7$ \\ $+ b_8 + b_9$ \\ $+ b_{10}$} & \shortstack[c]{$-b_9$ \\ \,} & \shortstack[c]{$-b_{10}$ \\ \,} \\
\shortstack[c]{$-b_2$ \\ \,} & \shortstack[c]{$-b_5$ \\ \,} & \shortstack[c]{$-b_9$ \\ \,} & \shortstack[c]{$m_2 + b_2$ \\ $+ b_5 + b_9$ \\ $+ b_{11} + b_{12}$} & \shortstack[c]{$-b_{15}$ \\ \,} \\
\shortstack[c]{$-b_3$ \\ \,} & \shortstack[c]{$-b_6$ \\ \,} & \shortstack[c]{$-b_{10}$ \\ \,} & \shortstack[c]{$-b_{15}$ \\ \,} & \shortstack[c]{$m_3 + b_3$ \\ $+ b_6 + b_{10}$ \\ $+ b_{13} + b_{14}$ \\ $+ b_{15}$}
\end{bmatrix}
\end{aligned}
\end{equation}

\paragraph{Damping Matrix}

\begin{equation}\label{Eq.damping_matrix_dimensional_combined}
\begin{aligned}
[C] =& 
\begin{bmatrix}
\shortstack{$C_1 + C_2$ \\ $+ C_3$} & -C_3 & \shortstack{$-c_1$ \\ \,} & \shortstack{$-c_2$ \\ \,} & \shortstack{$-c_3$ \\ \,} \\
-C_3 & \shortstack{$C_3 + C_4$ \\ $+ C_5 + c_4$ \\ $+ c_5 + c_6$} & \shortstack{$-c_4$ \\ \,} & \shortstack{$-c_5$ \\ \,} & \shortstack{$-c_6$ \\ \,} \\
-c_1 & -c_2 & \shortstack{$c_1 + c_4$ \\ $+ c_7 + c_8$ \\ $+ c_9 + c_{10}$} & \shortstack{$-c_9$ \\ \,} & \shortstack{$-c_{10}$ \\ \,} \\
-c_2 & -c_5 & -c_9 & \shortstack{$c_2 + c_5$ \\ $+ c_9 + c_{11}$ \\ $+ c_{12} + c_{15}$} & \shortstack{$-c_{15}$ \\ \,} \\
-c_3 & -c_{6} & -c_{10} & -c_{15} & \shortstack{$c_3 + c_6$ \\ $+ c_{10} + c_{13}$ \\ $+ c_{14} + c_{15}$}
\end{bmatrix}
\end{aligned}
\end{equation}

\paragraph{Stiffness Matrix}

\begin{equation}\label{Eq.stiffness_matrix_dimensional_combined}
\begin{aligned}
[K] =& 
\begin{bmatrix}
\shortstack{$K_1 + K_2$ \\ $+ K_3$} & -K_3 & \shortstack{$-k_1$ \\ \,} & \shortstack{$-k_2$ \\ \,} & \shortstack{$-k_3$ \\ \,} \\
-K_3 & \shortstack{$K_3 + K_4$ \\ $+ K_5 + k_4$ \\ $+ k_5 + k_6$} & \shortstack{$-k_4$ \\ \,} & \shortstack{$-k_5$ \\ \,} & \shortstack{$-k_6$ \\ \,} \\
-k_1 & -k_2 & \shortstack{$k_1 + k_4$ \\ $+ k_7 + k_8$ \\ $+ k_9 + k_{10}$} & \shortstack{$-k_9$ \\ \,} & \shortstack{$-k_{10}$ \\ \,} \\
-k_2 & -k_5 & -k_9 & \shortstack{$k_2 + k_5$ \\ $+ k_9 + k_{11}$ \\ $+ k_{12} + k_{15}$} & \shortstack{$-k_{15}$ \\ \,} \\
-k_3 & -k_{6} & -k_{10} & -k_{15} & \shortstack{$k_3 + k_6$ \\ $+ k_{10} + k_{13}$ \\ $+ k_{14} + k_{15}$}
\end{bmatrix}
\end{aligned}
\end{equation}

\paragraph{Force Vector}

\begin{equation}\label{Eq.force_vector_dimensional_combined}
\begin{aligned}
[F] = & \begin{bmatrix}
F_1(t) + C_1 \dot{U}_{low} + C_2 \dot{U}_{upp} + K_1 U_{low} + K_2 U_{upp} \\
F_2(t) + C_4 \dot{U}_{low} + C_5 \dot{U}_{upp} + K_4 U_{low} + K_5 U_{upp} \\
\beta_7 \ddot{U}_{low} + \beta_8 \ddot{U}_{upp} + 2 \zeta_{dc} \omega_{dc} (\nu_7 \dot{U}_{low} + \nu_8 \dot{U}_{upp}) + \omega_{dc}^2 (\lambda_7 U_{low} + \lambda_8 U_{upp}) \\
\beta_{11} \ddot{U}_{low} + \beta_{12} \ddot{U}_{upp} + 2 \zeta_{dc} \omega_{dc} (\nu_{11} \dot{U}_{low} + \nu_{12} \dot{U}_{upp}) + \omega_{dc}^2 (\lambda_{11} U_{low} + \lambda_{12} U_{upp}) \\
\beta_{13} \ddot{U}_{low} + \beta_{14} \ddot{U}_{upp} + 2 \zeta_{dc} \omega_{dc} (\nu_{13} \dot{U}_{low} + \nu_{14} \dot{U}_{upp}) + \omega_{dc}^2 (\lambda_{13} U_{low} + \lambda_{14} U_{upp})
\end{bmatrix}
\end{aligned}
\end{equation}

\subsubsection{Dimensionless Form}

To simplify analysis, the system is normalized using dimensionless parameters listed in Table~\ref{Tab:dimensionless.table}.


\begin{table}[h!]
\centering
\caption{Dimensionless Parameters for System Normalization}
\label{Tab:dimensionless.table}
\begin{tabular}{lcc}
\toprule
\textbf{Parameter Group} & \textbf{Parameter} & \textbf{Definition} \\
\midrule
\textbf{Mass Ratios} & \( \Gamma \) & \( \Gamma = \frac{M_2}{M_1} \) \\
 & \( \mu_i \) & \( \mu_i = \frac{m_i}{M_1} \) \\
\addlinespace
\textbf{Inertial Coupling Ratios} & \( \beta_i \) & \( \beta_i = \frac{b_i}{M_1} \) \\
\addlinespace
\textbf{Damping Ratios} & \( \mathcal{N}_i \) & \( \mathcal{N}_i = \frac{C_i}{C_1} \) \\
 & \( \nu_i \) & \( \nu_i = \frac{c_i}{C_1} \) \\
\addlinespace
\textbf{Stiffness Ratios} & \( \Lambda_i \) & \( \Lambda_i = \frac{K_i}{K_1} \) \\
 & \( \lambda_i \) & \( \lambda_i = \frac{k_i}{K_1} \) \\
\addlinespace
\textbf{Decoupled Primary System} & \( \omega_{dc} \) & \( \omega_{dc} = \sqrt{\frac{K_1}{M_1}} \) \\
 & \( \zeta_{dc} \) & \( \zeta_{dc} = \frac{C_1}{2 M_1 \omega_{dc}} \) \\
\bottomrule
\end{tabular}
\end{table}

Using these parameters, the dimensionless equations of motion are expressed as:

\begin{equation}\label{Eq.EOM_dimensionless}
\mathbf{\bar{M}} \ddot{\mathbf{q}} + 2 \zeta_{dc} \omega_{dc} \mathbf{\bar{C}} \dot{\mathbf{q}} + \omega_{dc}^2 \mathbf{\bar{K}} \mathbf{q} = \mathbf{\bar{F}}(t)
\end{equation}

The dimensionless mass, damping, and stiffness matrices, along with the force vector, are defined in Equations \eqref{Eq.mass_matrix_dimensionless} to \eqref{Eq.force_vector_dimensionless}.

\paragraph{Dimensionless Mass Matrix}

\begin{equation}\label{Eq.mass_matrix_dimensionless}
\begin{aligned}
[\bar{M}] =& 
\begin{bmatrix}
\shortstack{$1 + \beta_1$ \\ $+ \beta_2 + \beta_3$} & 0 & \shortstack{$-\beta_1$ \\ \,} & \shortstack{$-\beta_2$ \\ \,} & \shortstack{$-\beta_3$ \\ \,} \\
0 & \shortstack{$\Gamma + \beta_4$ \\ $+ \beta_5 + \beta_6$} & \shortstack{$-\beta_4$ \\ \,} & \shortstack{$-\beta_5$ \\ \,} & \shortstack{$-\beta_6$ \\ \,} \\
-\beta_1 & -\beta_4 & \shortstack{$\mu_1 + \beta_1$ \\ $+ \beta_4 + \beta_7$ \\ $+ \beta_8 + \beta_9$ \\ $+ \beta_{10}$} & \shortstack{$-\beta_9$ \\ \,} & \shortstack{$-\beta_{10}$ \\ \,} \\
-\beta_2 & -\beta_5 & -\beta_9 & \shortstack{$\mu_2 + \beta_2$ \\ $+ \beta_5 + \beta_9$ \\ $+ \beta_{11} + \beta_{12}$} & \shortstack{$-\beta_{15}$ \\ \,} \\
-\beta_3 & -\beta_6 & -\beta_{10} & -\beta_{15} & \shortstack{$\mu_3 + \beta_3$ \\ $+ \beta_6 + \beta_{10}$ \\ $+ \beta_{13} + \beta_{14}$ \\ $+ \beta_{15}$}
\end{bmatrix}
\end{aligned}
\end{equation}

\paragraph{Dimensionless Damping Matrix}

\begin{equation}\label{Eq.damping_matrix_dimensionless}
\begin{aligned}
[\bar{C}] =& 
\begin{bmatrix}
\shortstack{$1 + \mathcal{N}_2$ \\ $+ \mathcal{N}_3 + \nu_1$ \\ $+ \nu_2 + \nu_3$} & -\mathcal{N}_3 & \shortstack{$-\nu_1$ \\ \,} & \shortstack{$-\nu_2$ \\ \,} & \shortstack{$-\nu_3$ \\ \,} \\
-\mathcal{N}_3 & \shortstack{$\mathcal{N}_3 + \mathcal{N}_4$ \\ $+ \mathcal{N}_5 + \nu_4$ \\ $+ \nu_5 + \nu_6$} & \shortstack{$-\nu_4$ \\ \,} & \shortstack{$-\nu_5$ \\ \,} & \shortstack{$-\nu_6$ \\ \,} \\
-\nu_1 & -\nu_2 & \shortstack{$\nu_1 + \nu_4$ \\ $+ \nu_7 + \nu_8$ \\ $+ \nu_9 + \nu_{10}$} & \shortstack{$-\nu_9$ \\ \,} & \shortstack{$-\nu_{10}$ \\ \,} \\
-\nu_2 & -\nu_5 & -\nu_9 & \shortstack{$\nu_2 + \nu_5$ \\ $+ \nu_9 + \nu_{11}$ \\ $+ \nu_{12} + \nu_{15}$} & \shortstack{$-\nu_{15}$ \\ \,} \\
-\nu_3 & -\nu_{6} & -\nu_{10} & -\nu_{15} & \shortstack{$\nu_3 + \nu_6$ \\ $+ \nu_{10} + \nu_{13}$ \\ $+ \nu_{14} + \nu_{15}$}
\end{bmatrix}
\end{aligned}
\end{equation}

\paragraph{Dimensionless Stiffness Matrix}

\begin{equation}\label{Eq.stiffness_matrix_dimensionless}
\begin{aligned}
[\bar{K}] =& 
\begin{bmatrix}
\shortstack{$1 + \Lambda_2$ \\ $+ \Lambda_3 + \lambda_1$ \\ $+ \lambda_2 + \lambda_3$} & -\Lambda_3 & \shortstack{$-\lambda_1$ \\ \,} & \shortstack{$-\lambda_2$ \\ \,} & \shortstack{$-\lambda_3$ \\ \,} \\
-\Lambda_3 & \shortstack{$\Lambda_3 + \Lambda_4$ \\ $+ \Lambda_5 + \lambda_4$ \\ $+ \lambda_5 + \lambda_6$} & \shortstack{$-\lambda_4$ \\ \,} & \shortstack{$-\lambda_5$ \\ \,} & \shortstack{$-\lambda_6$ \\ \,} \\
-\lambda_1 & -\lambda_2 & \shortstack{$\lambda_1 + \lambda_4$ \\ $+ \lambda_7 + \lambda_8$ \\ $+ \lambda_9 + \lambda_{10}$} & \shortstack{$-\lambda_9$ \\ \,} & \shortstack{$-\lambda_{10}$ \\ \,} \\
-\lambda_2 & -\lambda_5 & -\lambda_9 & \shortstack{$\lambda_2 + \lambda_5$ \\ $+ \lambda_9 + \lambda_{11}$ \\ $+ \lambda_{12} + \lambda_{15}$} & \shortstack{$-\lambda_{15}$ \\ \,} \\
-\lambda_3 & -\lambda_{6} & -\lambda_{10} & -\lambda_{15} & \shortstack{$\lambda_3 + \lambda_6$ \\ $+ \lambda_{10} + \lambda_{13}$ \\ $+ \lambda_{14} + \lambda_{15}$}
\end{bmatrix}
\end{aligned}
\end{equation}

\paragraph{Dimensionless Force Vector}

\begin{equation}\label{Eq.force_vector_dimensionless}
\begin{aligned}
[\bar{F}] = & \begin{bmatrix}
\frac{F_1(t)}{M_1} + 2 \zeta_{dc} \omega_{dc} (\dot{U}_{low} + \mathcal{N}_2 \dot{U}_{upp}) + \omega_{dc}^2 (U_{low} + \Lambda_2 U_{upp})\\
\frac{F_2(t)}{M_1} + 2 \zeta_{dc} \omega_{dc} (\mathcal{N}_4 \dot{U}_{low} + \mathcal{N}_5 \dot{U}_{upp}) + \omega_{dc}^2 (\Lambda_4 U_{low} + \Lambda_5 U_{upp}) \\
\beta_7 \ddot{U}_{low} +  \beta_8 \ddot{U}_{upp} +  2 \zeta_{dc} \omega_{dc} (\nu_7 \dot{U}_{low} + \nu_8 \dot{U}_{upp}) +\omega_{dc}^2 (\lambda_7 U_{low} + \lambda_8 U_{upp})\\
\beta_{11} \ddot{U}_{low} +  \beta_{12} \ddot{U}_{upp} +  2 \zeta_{dc} \omega_{dc} (\nu_{11} \dot{U}_{low} + \nu_{12} \dot{U}_{upp}) +\omega_{dc}^2 (\lambda_{11} U_{low} + \lambda_{12} U_{upp})\\
\beta_{13} \ddot{U}_{low} +  \beta_{14} \ddot{U}_{upp} +  2 \zeta_{dc} \omega_{dc} (\nu_{13} \dot{U}_{low} + \nu_{14} \dot{U}_{upp}) +\omega_{dc}^2 (\lambda_{13} U_{low} + \lambda_{14} U_{upp})
\end{bmatrix}
\end{aligned}
\end{equation}

\paragraph{Semi-Analytical Solutions for Harmonic Excitation}

The semi-analytical method assumes harmonic excitation and synchronized motion. The system response is expressed as:

\begin{align}\label{Eq.harmonic.solution.2dof3dof}
    \begin{bmatrix}
        U_1(t) \\
        U_2(t) \\
        u_1(t) \\
        u_2(t) \\
        u_3(t)
    \end{bmatrix} =
    \begin{bmatrix}
        A_1 \\
        A_2 \\
        a_1 \\
        a_2 \\
        a_3
    \end{bmatrix} e^{j \omega t}
\end{align}

where $A_1, A_2$ are the vibration amplitudes of the primary masses and $a_1, a_2, a_3$ are the amplitudes of the DVA masses.

The harmonic excitations are defined as:

\begin{align}\label{Eq.harmonic.excitations.2dof3dof}
    \begin{split}
        F_1(t) &= F_1 e^{j \omega t} \\
        F_2(t) &= F_2 e^{j \omega t} \\
        U_{Low}(t) &= A_{Low} e^{j \omega t} \\
        U_{Up}(t) &= A_{Up} e^{j \omega t}
    \end{split}
\end{align}

Substituting the harmonic solutions into the equations of motion yields the frequency domain formulation:

\begin{align}\label{Eq.frequency.domain.2dof3dof}
    \begin{bmatrix}
        A_1 \\
        A_2 \\
        a_1 \\
        a_2 \\
        a_3
    \end{bmatrix} = \omega_{dc}^2 \left( -\Omega^2 \mathbf{M} + j 2 \zeta_{dc} \Omega \mathbf{C} + \mathbf{K} \right)^{-1} \mathbf{F}
\end{align}

where $\mathbf{F}$ is the complex amplitude vector of the forcing function.



\subsection{Optimization Problem Formulation}

Building upon the complex 2DOF-3DOF mechanical system described in the previous section, the optimization problem is formulated to find optimal Dynamic Vibration Absorber (DVA) parameters that minimize deviations from desired system performance characteristics. The system comprises 48 independent design parameters distributed across 15 mass coupling coefficients ($\beta_1$ through $\beta_{15}$), 15 stiffness parameters ($\lambda_1$ through $\lambda_{15}$), 15 damping parameters ($\nu_1$ through $\nu_{15}$), and 3 DVA mass parameters ($\mu_1$, $\mu_2$, $\mu_3$).

The optimization problem can be mathematically stated as:

\begin{equation}\label{Eq.optimization_problem}
\begin{aligned}
\min_{\mathbf{x}} \quad & f(\mathbf{x}) = f_{primary}(\mathbf{x}) + f_{sparsity}(\mathbf{x}) + f_{error}(\mathbf{x}) \\
\text{subject to} \quad & \mathbf{x}_L \leq \mathbf{x} \leq \mathbf{x}_U \\
\end{aligned}
\end{equation}

where:
\begin{itemize}
    \item $\mathbf{x} \in \mathbb{R}^{48}$ is the design parameter vector
    \item $\mathbf{x}_L, \mathbf{x}_U \in \mathbb{R}^{48}$ are the lower and upper parameter bounds
    \item $f_{primary}(\mathbf{x})$, $f_{sparsity}(\mathbf{x})$, $f_{error}(\mathbf{x})$ are the objective function components
\end{itemize}

\subsubsection{Objective Function Definition}

The overall objective function is a weighted sum of three distinct components, each addressing different aspects of the optimization problem:

\begin{equation}\label{Eq.objective_function_detailed}
f(\mathbf{x}) = f_{primary}(\mathbf{x}) + f_{sparsity}(\mathbf{x}) + f_{error}(\mathbf{x})
\end{equation}

where each component is precisely defined and serves a specific purpose in the optimization process.

\textbf{Primary Objective Function ($f_{primary}(\mathbf{x})$):} The primary objective measures the deviation of the system's singular response from the optimal target value of 1.0:

\begin{equation}\label{Eq.primary_objective_detailed}
f_{primary}(\mathbf{x}) = \left| C_s(\mathbf{x}) - 1.0 \right|
\end{equation}

where the singular criteria $C_s(\mathbf{x})$ is computed as:

\begin{equation}\label{Eq.singular_response_detailed}
C_s(\mathbf{x}) = \sum_{i=1}^{5} CM_i(\mathbf{x})
\end{equation}

and $CM_i(\mathbf{x})$ is the composite measure for mass $i$:

\begin{equation}\label{Eq.composite_measure_detailed}
CM_i(\mathbf{x}) = \sum_{j} w_{ij} \cdot \frac{a_{ij}(\mathbf{x})}{t_{ij}}
\end{equation}

The composite measure integrates multiple performance criteria for each mass, where:
\begin{itemize}
    \item $w_{ij}$: Weight coefficient for criterion $j$ of mass $i$
    \item $a_{ij}(\mathbf{x})$: Actual performance value for criterion $j$ of mass $i$
    \item $t_{ij}$: Target performance value for criterion $j$ of mass $i$
\end{itemize}

The performance criteria include peak positions, peak values, bandwidths, slopes, and area under the curve, as extracted from the FRF analysis. The weights allow prioritization of different performance aspects, with typical values ranging from 0.05 to 1.0 depending on the importance of each criterion.

\textbf{Sparsity Penalty Function ($f_{sparsity}(\mathbf{x})$):} The sparsity penalty encourages solutions with smaller parameter magnitudes:

\begin{equation}\label{Eq.sparsity_penalty_detailed}
f_{sparsity}(\mathbf{x}) = \alpha \sum_{k=1}^{48} |x_k|
\end{equation}

where:
\begin{itemize}
    \item $\alpha$ is the sparsity weight coefficient 
    \item $x_k$ represents the $k$-th design parameter
    \item The L1 regularization promotes sparse solutions by penalizing non-zero parameter values
\end{itemize}

This function serves multiple purposes:

\begin{enumerate}
    \item \textbf{Regularization}: Prevents overfitting to specific frequency ranges by discouraging overly complex parameter combinations
    \item \textbf{Practical Implementation}: Encourages simpler DVA configurations that are easier to manufacture and maintain
    \item \textbf{Robustness Enhancement}: Reduces sensitivity to parameter variations in real-world applications
\end{enumerate}

\textbf{Percentage Error Component ($f_{error}(\mathbf{x})$):} The percentage error component captures detailed performance deviations:

\begin{align}\label{Eq.percentage_error_detailed}
f_{error}(\mathbf{x}) &= \frac{1}{\gamma} \sum_{i} \sum_{j} \left| PD_{ij}(\mathbf{x}) \right|\\
PD_{ij}(\mathbf{x}) &= \left( \frac{a_{ij}(\mathbf{x}) - t_{ij}}{|t_{ij}|} \right) \times 100\%
\end{align}
where:
\begin{itemize}
    \item $\gamma$ is the scaling factor (default: 1000)
    \item $PD_{ij}(\mathbf{x})$ is the percentage difference for criterion $j$ of mass $i$
    \item The absolute value prevents cancellation between positive and negative errors
\end{itemize}

This component ensures comprehensive evaluation by:
\begin{itemize}
    \item Capturing all performance criteria deviations in the objective function
    \item Enabling consideration of both primary response characteristics and detailed performance metrics
    \item Allowing different criteria to have varying importance through target value specification
    \item Providing a normalized error measure that can be compared across different criteria types
    \item Balancing the contribution of detailed metrics with the primary objective through the scaling factor $\gamma$
\end{itemize}

The scaling factor $\gamma$ plays a crucial role in balancing the contribution of the percentage error component with the other objectives. A larger $\gamma$ reduces the influence of detailed percentage errors, while a smaller $\gamma$ increases their importance in the overall optimization.

\subsubsection{Performance Criteria Hierarchy}
The multi-objective framework evaluates performance across several hierarchical categories:

\begin{enumerate}
    \item \textbf{Modal Performance Criteria:}
    \begin{itemize}
        \item Peak positions ($\omega_{peak,i}$): Target resonant frequencies for each mass
        \item Peak values ($A_{peak,i}$): Target response amplitudes at resonant frequencies
        \item Critical for modal alignment and vibration control effectiveness
    \end{itemize}
    

    \item \textbf{Inter-Modal Criteria:}
    \begin{itemize}
        \item Bandwidths ($\Delta\omega_{i,j}$): Target frequency ranges between resonances
        \item Slopes ($s_{i,j}$): Rate of change between peaks
        \item Affects system stability and control bandwidth
    \end{itemize}

    \item \textbf{Global Response Criteria:}
    \begin{itemize}
        \item Area under curve: Total response energy across frequency range
        \item Maximum slope: Maximum rate of amplitude change
        \item Provides overall system response characteristics
    \end{itemize}
\end{enumerate}

\textbf{Weight Assignment Strategy:} The assignment of weights to each objective function is a critical aspect of the optimization process, as it directly influences the balance between different performance criteria in the total fitness function. In this methodology, the program calculates $C_s - 1$ as one of the objective functions, and it is important to ensure that the contribution of each objective is properly normalized.

To achieve a meaningful and interpretable fitness value, it is recommended that the sum of all weights assigned to the objectives does not exceed 1, and ideally, all weights should sum exactly to 1. This normalization ensures that each objective's influence is proportional and that the overall fitness function is calculated correctly, preventing any single criterion from dominating the optimization process due to disproportionate weighting.

The general structure for assigning weights is as follows:

\begin{equation}\label{Eq.weight_hierarchy_detailed}
w_{ij} = w_i \cdot w_{base,j}
\end{equation}

where:
\begin{itemize}
    \item $w_i$: Weight associated with a specific mass or subsystem (often set to 1.0 for uniform treatment, but can be adjusted for prioritization)
    \item $w_{base,j}$: Base weight assigned to each criterion type $j$ (e.g., peak position, peak value, bandwidth, slope, area, etc.)
\end{itemize}

The sum of all $w_{ij}$ across all masses and criteria should satisfy:
\begin{equation}
\sum_{i,j} w_{ij} \leq 1
\end{equation}
and, for best normalization, it is preferable to enforce
\begin{equation}
\sum_{i,j} w_{ij} = 1
\end{equation}

This approach ensures that the fitness function remains consistent and interpretable, especially when combining $C_s - 1$ with other objectives. However, since the program is open-source, users have the flexibility to modify the weighting scheme as needed for their specific application or research focus. Adjusting the weights allows for custom prioritization of objectives, but care should be taken to maintain the normalization condition for optimal performance and comparability of results.

\textbf{Objective Function Interactions:} 

In multi-objective optimization, especially in the context of genetic algorithms for engineering design, the total objective function $f_{total}(\mathbf{x})$ is typically composed of several distinct terms, each representing a different aspect of system performance or a different design goal. These terms may include, for example, a primary performance objective (such as minimizing vibration amplitude), a sparsity-promoting penalty (to encourage simpler or more efficient designs), and an error or constraint penalty (to penalize infeasible or undesirable solutions).


\subsubsection{Fitness Evaluation through FRF Analysis}


\textbf{FRF Mathematical Foundation:} The FRF analysis is based on the frequency domain representation of the system dynamics:

\begin{equation}\label{Eq.frequency_domain_system_detailed}
\mathbf{X}(\omega) = \mathbf{G}(\omega) \mathbf{F}(\omega)
\end{equation}

where:
\begin{itemize}
    \item $\mathbf{X}(\omega) \in \mathbb{C}^{5}$: Displacementresponse vector (5 DOF)
    \item $\mathbf{G}(\omega) \in \mathbb{C}^{5 \times 5}$: Frequency response function matrix
    \item $\mathbf{F}(\omega) \in \mathbb{C}^{5}$: Forcing function vector including external and base excitations
\end{itemize}

The frequency response function matrix is computed as:

\begin{equation}\label{Eq.frf_matrix_detailed}
\mathbf{G}(\omega) = \left[ -\omega^2 \mathbf{M} + j\omega \mathbf{C} + \mathbf{K} \right]^{-1}
\end{equation}

where $\mathbf{M}$, $\mathbf{C}$, $\mathbf{K}$ are the system matrices defined in the mechanical system formulation.


\textbf{Performance Metric Calculations:} Each mass response undergoes comprehensive analysis:

\begin{enumerate}
    \item \textbf{Peak Frequency (X-Value) Analysis:}
    \begin{equation}\label{Eq.peak_frequency_analysis_detailed}
    \omega_{peak,i} = \omega_{j^*} \quad \text{where} \quad j^* = \arg\max_{j} |X_i(\omega_j)|, \quad \forall i = 1,\ldots,5
    \end{equation}

    \item \textbf{Peak Amplitude (Y-Value) Analysis:}
    \begin{equation}\label{Eq.peak_amplitude_analysis_detailed}
    A_{peak,i} = |X_i(\omega_{peak,i})| \quad \forall i = 1,\ldots,5
    \end{equation}

    \item \textbf{Bandwidth Analysis:}
    \begin{equation}\label{Eq.bandwidth_analysis_detailed}
    \Delta\omega_{i,j} = |\omega_{peak,j} - \omega_{peak,i}| \quad \forall i < j
    \end{equation}

    \item \textbf{Slope Analysis:}
    \begin{equation}\label{Eq.slope_analysis_detailed}
    s_{i,j} = \frac{A_{peak,j} - A_{peak,i}}{\omega_{peak,j} - \omega_{peak,i}} \quad \forall i < j
    \end{equation}

    \item \textbf{Area Analysis:}
    \begin{equation}\label{Eq.area_analysis_detailed}
    Area_i = \int_{\omega_{start}}^{\omega_{end}} |X_i(\omega)| d\omega \quad \forall i = 1,\ldots,5
\end{equation}
\end{enumerate}

The area integral is computed using Simpson's rule for numerical accuracy:

\begin{equation}\label{Eq.area_simpson_detailed}
Area_i \approx \frac{h}{3} \left[|X_i(\omega_0)| + 4\sum_{k=1}^{n/2} |X_i(\omega_{2k-1})| + 2\sum_{k=1}^{n/2-1} |X_i(\omega_{2k})| + |X_i(\omega_n)| \right]
\end{equation}

where $h$ is the frequency step size and $n$ is the number of frequency points.


\section{Traditional Genetic Algorithm Methodology}

    \subsection{Algorithmic Framework}
    The baseline GA follows the classic evolve–evaluate–select loop with real-valued representations tailored to the DVA design vector $\mathbf{x}$ and FRF-based fitness in Eq.~\eqref{Eq.objective_function_detailed}. Individuals are length-$P$ floating vectors, bounds are enforced by projection, and fixed parameters are respected.

    \paragraph{Representation and constraints}
    \begin{itemize}
        \item \textbf{Genome}: $\mathbf{x}=[x_1,\dots,x_P]^\top$ with $x_i\in[\theta_i^{\min},\theta_i^{\max}]$; fixed parameters are pinned to their fixed values.
        \item \textbf{Projection to bounds}: after any variation, each gene is clamped
        \begin{equation}\label{Eq.bound_projection}
            x_i \leftarrow \min\{\, \theta_i^{\max},\; \max\{\theta_i^{\min},\; x_i\}\,\},\quad i=1,\dots,P.
        \end{equation}
    \end{itemize}

    \paragraph{Fitness evaluation}
    Each individual is evaluated by the FRF pipeline (Sec.~\ref{subsubsec:chosen_performance_criteria_combined}), yielding the composite singular criterion $C_s$ in Eq.~\eqref{Eq.singular_response_detailed} and the detailed percentage differences in Eq.~\eqref{Eq.percentage_error_detailed}. The total fitness is Eq.~\eqref{Eq.objective_function_detailed}:
    \begin{equation}
        f(\mathbf{x})=\big|C_s(\mathbf{x})-1.0\big|\; +\; \alpha\sum_{k=1}^{P}|x_k|\; +\; \frac{1}{\gamma}\sum_{i,j}\big|PD_{ij}(\mathbf{x})\big|.
    \end{equation}

    \paragraph{Selection, crossover, mutation, replacement}
    \begin{itemize}
        \item \textbf{Selection}: tournament selection with size 3 (balance pressure and diversity).
        \item \textbf{Crossover}: \emph{Blend} crossover (BLX-\(\alpha\)) with $\alpha=0.5$ acts gene-wise on pairs.
        \item \textbf{Mutation}: per-gene perturbation with probability $\mathrm{indpb}=0.1$, using a uniform perturbation bounded to 10\% of each gene span; then projection (Eq.~\ref{Eq.bound_projection}).
        \item \textbf{Replacement}: generational replacement of the full population with the offspring.
    \end{itemize}

    \paragraph{Termination}
    The loop terminates when either (i) the best fitness reaches the tolerance $f_{\min}\le\texttt{ga\_tol}$, or (ii) the generation budget $G_{\max}$ is exhausted.

    \subsection{Baseline GA for DVA Design}
    \begin{algorithm}[H]
    \caption{Baseline Genetic Algorithm for DVA parameter optimization}
    \begin{algorithmic}[1]
    \REQUIRE Bounds $[\theta^{\min},\theta^{\max}]$, fixed mask/values, population size $N$, generations $G_{\max}$, $\text{cxpb}$, $\text{mutpb}$, $\text{indpb}$, tolerance $\tau$
    \STATE Initialize population $\mathcal{P}_0$ of $N$ individuals uniformly in bounds; set fixed genes.
    \FOR{$g=1$ to $G_{\max}$}
        \STATE Evaluate each $\mathbf{x}\in\mathcal{P}_{g-1}$ via FRF to compute $f(\mathbf{x})$.
        \IF{$\min_{\mathbf{x}\in\mathcal{P}_{g-1}} f(\mathbf{x})\le\tau$} \textbf{break} \ENDIF
        \STATE $\mathcal{M}\leftarrow$ tournament-select $N$ parents from $\mathcal{P}_{g-1}$ (size 3).
        \STATE Form offspring $\mathcal{O}$ by pairing parents and applying BLX-0.5 with prob. $\text{cxpb}$.
        \FOR{each offspring $\mathbf{x}\in\mathcal{O}$}
            \IF{$\text{rand}<\text{mutpb}$} mutate each gene with prob. $\text{indpb}$ by a uniform perturbation in $\pm 0.1\,(\theta_i^{\max}-\theta_i^{\min})$; enforce fixed genes and project with Eq.~\eqref{Eq.bound_projection} \ENDIF
        \ENDFOR
        \STATE Evaluate invalid individuals in $\mathcal{O}$; set $\mathcal{P}_g\leftarrow\mathcal{O}$.
    \ENDFOR
    \STATE \textbf{return} best individual and fitness.
    \end{algorithmic}
    \end{algorithm}

    \subsection{Complexity and limitations}
    Let $C_{\text{FRF}}$ be the average cost of one FRF evaluation and $N$ the population size. The time per generation is $\Theta(N\,C_{\text{FRF}})+\mathcal{O}(N)$; crossover/mutation are linear. Classical drawbacks are: (i) premature convergence under fixed operators, (ii) sensitivity to hyperparameters, (iii) costly fitness evaluations, and (iv) random seeding that may under-sample high-quality regions. These motivate the advanced controllers and screening introduced next.




\section{Advanced Genetic Algorithm Methodology}
    \subsection{Overview of Advanced Features}
    DeVana extends the baseline GA with controllers and samplers that adapt online: (i) \textbf{adaptive operator control} via an ML bandit or a lightweight RL agent that tunes crossover $\text{cxpb}$, mutation $\text{mutpb}$, and population size $N$ generation-by-generation; (ii) \textbf{advanced seeding} with Sobol/LHS low-discrepancy designs and a \textbf{NeuralSeeder} that learns promising regions using UCB or EI; and (iii) \textbf{surrogate-assisted screening} that pre-filters offspring using a KNN surrogate with novelty-based exploration. All mechanisms respect fixed parameters and bounds.

    \subsection{Machine Learning Bandit Controller for Parameter Adaptation}
    The controller defines a discrete action space $\mathcal{A}=\{(\delta_{\text{cx}},\delta_{\\mu},\pi)\}$ with relative changes to $\text{cxpb}$ and $\text{mutpb}$ and a multiplier $\pi$ for population size. At generation $t$, it selects
    \begin{equation}
        a_t\;=\;\arg\max_{a\in\mathcal{A}}\Big[ \hat{R}_t(a)\; +\; c\,\sqrt{\tfrac{\ln t}{N_t(a)}} \Big], \label{Eq.ucb}
    \end{equation}
    where $\hat{R}_t(a)$ is the average reward observed for action $a$, $N_t(a)$ its count, and $c>0$ controls exploration. The chosen action updates
    \begin{align}
        \text{cxpb}_{t+1}&=\operatorname{clip}(\text{cxpb}_t\,(1+\delta_{\text{cx}}),\;[\text{cxpb}_{\min},\text{cxpb}_{\max}]),\\
        \text{mutpb}_{t+1}&=\operatorname{clip}(\text{mutpb}_t\,(1+\delta_{\mu}),\;[\text{mutpb}_{\min},\text{mutpb}_{\max}]),\\
        N_{t+1}&=\operatorname{round}(\operatorname{clip}(\pi\,N_t,\;[N_{\min},N_{\max}])).
    \end{align}
    \paragraph{Reward shaping}
    Consistent with the implementation, the reward balances improvement, speed, diversity, and effort:
    \begin{equation}\label{Eq.bandit_reward}
        R_t\;=\;\frac{\max\{0,\,f^{\star}_{t-1}-f^{\star}_{t}\}}{\max\{\epsilon,\,T_t\}}\,\frac{1}{\max\{1,\,E_t\}}\; -\; w_{\text{div}}\,\big|\mathrm{CV}_t-\mathrm{CV}^{\text{target}}\big|,
    \end{equation}
    where $f^{\star}_t$ is the best fitness in generation $t$, $T_t$ is generation time, $E_t$ the number of evaluations, and $\mathrm{CV}_t=\tfrac{\sigma_t}{|\mu_t|+\epsilon}$ is the coefficient of variation of fitness. A blended estimator $\tilde R_t= w_{\text{hist}}\,\hat{R}_{t-1}(a)+w_{\text{cur}}\,R_t$ stabilizes updates.

    \begin{algorithm}[H]
    \caption{ML-Bandit controller (per generation)}
    \begin{algorithmic}[1]
    \STATE Compute statistics $\mu_t,\sigma_t,f^{\star}_t,T_t,E_t$ from the finished generation.
    \STATE For each $a\in\mathcal{A}$, compute UCB score via Eq.~\eqref{Eq.ucb}; select $a_t$.
    \STATE Update $(\text{cxpb},\text{mutpb},N)$ by the selected action with clipping.
    \STATE Observe $R_t$ via Eq.~\eqref{Eq.bandit_reward}, update counts and averages for $a_t$.
    \end{algorithmic}
    \end{algorithm}

    \subsection{Reinforcement Learning-Based Control of GA Parameters}
    A compact Q-learning agent with an $\epsilon$-greedy policy selects actions $a\in\mathcal{A}$ conditioned on a coarse state $s$ (e.g., improvement/no-improvement). The update is
    \begin{equation}
        Q(s,a)\leftarrow Q(s,a)+\alpha\,\big(R+\gamma\max_{a'}Q(s',a')-Q(s,a)\big),
    \end{equation}
    with learning rate $\alpha$, discount $\gamma$, and exploration $\epsilon$ decayed each generation. Actions map to the same $(\delta_{\text{cx}},\delta_{\mu},\pi)$ triplets and are clipped to guardrails as above.

    \begin{algorithm}[H]
    \caption{RL controller (per generation)}
    \begin{algorithmic}[1]
    \STATE Observe state $s_t$ (e.g., $\mathbb{1}[f^{\star}_t<f^{\star}_{t-1}]$).
    \STATE With prob. $\epsilon$ select random $a_t$, else $\arg\max_a Q(s_t,a)$.
    \STATE Apply $a_t$ to update $(\text{cxpb},\text{mutpb},N)$ with clipping.
    \STATE Receive reward $R_t$ per Eq.~\eqref{Eq.bandit_reward}, observe $s_{t+1}$; update $Q$.
    \STATE Decay $\epsilon\leftarrow\epsilon\cdot\epsilon_{\text{decay}}$.
    \end{algorithmic}
    \end{algorithm}

    \subsection{Sobol and Latin Hypercube Seeding}
    For $d=P$ free parameters with bounds $\ell_i,u_i$, a QMC engine generates $m$ points $\mathbf{z}_k\in[0,1]^d$ using either Sobol or LHS, then scales
    \begin{equation}
        x_{k,i}\;=\;\ell_i + z_{k,i}\,(u_i-\ell_i),\quad i=1,\dots,d,\;k=1,\dots,m,
    \end{equation}
    and overwrites fixed coordinates. QMC seeding improves space-filling vs. pure random initialization.

    \subsection{Neural Network-Based Population Seeding (NeuralSeeder)}
    The NeuralSeeder maintains a dataset $\mathcal{D}=\{(\mathbf{x}^{(n)}, y^{(n)})\}$ of evaluated points and trains a lightweight ensemble to model $y\approx f(\mathbf{x})$. It proposes candidates by optimizing an acquisition function such as UCB or EI over a candidate pool, with exploration fraction $\epsilon$ and UCB scale $\beta$ adapted online:
    \begin{align}\label{eq:UCBEI}
        \text{UCB}(\mathbf{x}) &= \mu(\mathbf{x}) - \beta\,\sigma(\mathbf{x}) \quad \text{(minimization)},\\
        \text{EI}(\mathbf{x}) &= \mathbb{E}\big[\max\{0, f^{\star}-Y(\mathbf{x})\}\big].
    \end{align}
    A diversity constraint (minimum distance in normalized space) avoids mode collapse. When the controller resizes the population, additional individuals are drawn from NeuralSeeder once it has seen enough data; otherwise it falls back to the configured seeding method.

    \subsection{Surrogate Model-Assisted Fitness Evaluation}
    To reduce FRF calls, a KNN surrogate pre-screens offspring. Let $\tilde{\mathbf{x}}$ be a candidate and define normalized coordinates $z_i=(x_i-\ell_i)/(u_i-\ell_i)$. The KNN predictor is
    \begin{equation}
        \hat f(\tilde{\mathbf{x}})= \frac{1}{k}\sum_{(\mathbf{x}^{(n)},y^{(n)})\in\mathcal{N}_k(\tilde{\mathbf{x}})} y^{(n)},\quad \mathcal{N}_k(\tilde{\mathbf{x}})=\arg\min_{\mathcal{S},\;|\mathcal{S}|=k}\sum_{(\mathbf{x},\cdot)\in\mathcal{S}}\lVert \mathbf{z}-\tilde{\mathbf{z}}\rVert_2.
    \end{equation}
    A pool of size $M=\lceil \phi\,Q\rceil$ (with $Q$ invalid offspring and pool factor $\phi\ge1$) is generated by cloning and light genetic operations; the top $q$ by $\hat f$ are \emph{exploited} and an \emph{explore} subset is added by maximizing novelty
    \begin{equation}
        \mathcal{N}(\tilde{\mathbf{x}})=\min_{\mathbf{x}^{(n)}\in\mathcal{D}}\lVert \tilde{\mathbf{z}}-\mathbf{z}^{(n)}\rVert_2.
    \end{equation}
    Only the chosen subset is evaluated by FRF; others are discarded, yielding substantial savings when FRF is expensive.

    \begin{algorithm}[H]
    \caption{Surrogate screening per generation}
    \begin{algorithmic}[1]
    \REQUIRE Invalid offspring set $\mathcal{U}$ of size $Q$, pool factor $\phi$, explore fraction $\eta$, KNN $k$
    \STATE Build pool $\mathcal{P}$ of size $M=\max\{Q,\lceil\phi Q\rceil\}$ by cloning $\mathcal{U}$ and applying light crossover/mutation (with bounds and fixed genes).
    \STATE Predict $\hat f$ for all $\mathbf{x}\in\mathcal{P}$; sort ascending.
    \STATE Select $q=Q$ exploit candidates with smallest $\hat f$.
    \STATE From the remainder, select $\eta q$ most novel by maximizing $\mathcal{N}(\cdot)$; add to chosen set.
    \STATE Evaluate FRF only on the chosen set; fill remaining offspring slots by best evaluated if needed.
    \end{algorithmic}
    \end{algorithm}

    \subsection{Legacy adaptive rates}
    As a fallback, a heuristic adapts $(\text{cxpb},\text{mutpb})$ based on diversity $\sigma/|\mu|$ and stagnation counter: low diversity increases mutation and decreases crossover, high diversity does the opposite, with periodic adjustments.


\section{Benchmarking and Robustness Analysis}
\label{sec:benchmarking}
DeVana instruments the optimization with detailed metrics to quantify efficiency and robustness across controllers and seeding strategies.

\subsection{Recorded metrics}
\begin{itemize}
    \item \textbf{Per generation}: best/mean/std fitness, population size, $(\text{cxpb},\text{mutpb})$, evaluations, time breakdown (selection/crossover/mutation/evaluation), total generation time.
    \item \textbf{System}: CPU\%, per-core CPU, memory usage/details, I/O counters, network, thread count.
    \item \textbf{Controllers}: action histories and rewards for ML-bandit/RL; rate/adaptation history.
\end{itemize}

\subsection{Key performance indicators (KPIs)}
Let $f^{\star}_g$ be best fitness at generation $g$ and $G$ the total generations.
\begin{align}
    g_{\tau} &= \min\{g:\ f^{\star}_g\le\tau\}\quad\text{(time-to-tolerance)},\\
    \mathrm{AUC} &= \sum_{g=1}^{G} f^{\star}_g\quad\text{(area under best-fitness curve; lower is better)},\\
    \bar T &= \frac{1}{G}\sum_{g=1}^{G} T_g,\quad \bar E = \frac{1}{G}\sum_{g=1}^{G} E_g,\\
    \Delta f_g &= \max\{0, f^{\star}_{g-1}-f^{\star}_g\},\quad \overline{\Delta f}=\frac{1}{G-1}\sum_{g=2}^{G}\Delta f_g.
\end{align}
Reliability is reported as the fraction of runs achieving $f^{\star}_G\le\tau$.

\subsection{Benchmark design}
We adopt a factorial study over controllers (Fixed/Adaptive/ML-Bandit/\gls{rl}) and seeders (Random/Sobol/LHS/Neural), with and without surrogate screening. For each cell, run $R$ replicates with different seeds; collect KPIs and resource metrics.

\paragraph{Ablation studies}
We perform ablations by enabling one advanced feature at a time: (i) adaptive controllers only; (ii) surrogate screening only; (iii) advanced seeding only; and (iv) combinations. This isolates the marginal impact of each component.

\paragraph{Sensitivity analyses}
We probe sensitivity to: (a) controller exploration constants, (b) surrogate pool factor $\phi$ and $k$, (c) NeuralSeeder exploration fraction and acquisition scale, and (d) objective weights. Outcomes are reported on ${g_\tau}$, AUC, and final best fitness.

\paragraph{External validity}
We validate across multiple mechanical scenarios (mass ratios, damping regimes, and different target criteria) to ensure conclusions generalize beyond a single configuration.

\subsection{Statistical analysis}
Report medians and IQRs across replicates; compare cells using non-parametric tests (e.g., Wilcoxon) and bootstrap 95\% CIs for median differences of AUC and final best fitness. When multiple factors are varied, use aligned ranks or a permutation-based two-way ANOVA analogue on AUC.

\subsection{Reporting}
Provide: (i) convergence traces with shaded IQR, (ii) boxplots of KPIs, (iii) Pareto plots of final fitness vs. wall-time/evaluations, (iv) controller rate/population trajectories, (v) resource timelines, and (vi) ablation/sensitivity summaries.

\section{Case studies and validation}
\label{sec:case-studies}
We demonstrate \softwareName{} on representative scenarios: (i) baseline 2DOF--3DOF configuration with two primary targets; (ii) high-damping regime; (iii) multi-peak realignment with stringent slope uniformity; and (iv) sensitivity to inerter couplings. For each case we compare the algorithm suite, with and without advanced features, and report KPIs and FRF visualizations.

\subsection{Experimental setup}
We fix bounds, targets, and weight schema per Sec.~\ref{subsubsec:chosen_performance_criteria_combined}. Each optimizer uses identical stopping criteria and evaluation budgets. Controllers and surrogates follow their default guardrails unless stated.

\subsection{Results overview}
Across cases, adaptive \gls{ga} controllers reduce time-to-tolerance and evaluations, surrogate screening yields the largest savings when FRF is most expensive, and NeuralSeeder improves early convergence and reliability. Results are detailed in Sec.~\ref{sec:benchmarking}.

\section{Statistical synthesis of recommended DVA parameter ranges}
\label{sec:statistical-ranges}

\subsection{Motivation and overview}
Multiple independent GA runs provide a sample of optimized parameter vectors that encode how the system converges under different random seeds, seeding strategies, and adaptive controllers. Rather than reporting a single point estimate, we construct statistically robust \emph{recommended ranges} for every design parameter. These ranges capture where high-performing solutions concentrate, improve interpretability, and guide robust engineering choices.

This section details a step-by-step, statistically grounded procedure that aggregates results from many runs and produces parameter ranges using several complementary criteria. It also defines how to compare, combine, and visualize these ranges, and it gives principled rules for selecting the final recommended interval per parameter.

\subsection{Data model and notation}
Let there be $R$ benchmark runs of the GA. Each run $r\in\{1,\dots,R\}$ returns:
\begin{itemize}
    \item Best solution vector $\mathbf{x}^{(r)} = [x^{(r)}_1,\dots, x^{(r)}_P]^\top$ of dimension $P$ (number of design parameters),
    \item Best fitness value $f^{(r)}$ (lower is better),
    \item Parameter name list $\{p_1,\dots,p_P\}$ consistent across runs.
\end{itemize}
For each parameter $p_j$ we collect the empirical sample
\begin{equation}
    \mathcal{X}_{p_j} = \{\, x^{(1)}_j, x^{(2)}_j, \dots, x^{(R)}_j \,\},\qquad j=1,\dots,P.
\end{equation}
We also define an optional performance subset using the $q$-quantile of fitness:
\begin{equation}
    \mathcal{I}_{q} = \{\, r:\ f^{(r)} \leq Q_f(q) \,\},\quad \mathcal{X}^{q}_{p_j}=\{\, x^{(r)}_j:\ r\in \mathcal{I}_{q}\,\},
\end{equation}
where $Q_f(q)$ is the $q$-quantile of $\{f^{(r)}\}_{r=1}^R$.

\subsection{Preprocessing and quality control}
Prior to range estimation we apply the following checks:
\begin{enumerate}[label=\textbf{P\arabic*}]
    \item \textbf{Validation}: discard runs with missing or incompatible dimension $P$, or non-finite entries.
    \item \textbf{Alignment}: ensure parameter names $\{p_j\}$ align across runs.
    \item \textbf{Optional winsorization}: for visual diagnostics only, we may clip the top/bottom 1\% to prevent extreme values from dominating plots; the statistical criteria below are robust and do not require it.
\end{enumerate}

\subsection{Range estimation criteria}
To provide statistically meaningful and robust recommended ranges for each design parameter $p_j$, we compute six complementary interval criteria. Each criterion yields an interval $[L^{(c)}_j,\,U^{(c)}_j]$ for parameter $p_j$, where $c$ indexes the criterion. These criteria are implemented in the GUI (see \texttt{codes/gui/main\_window/ga\_mixin.py}, e.g., \texttt{\_iqr\_range}, \texttt{\_p5\_p95}, \texttt{\_tukey\_whisker}, \texttt{\_shortest\_interval}, \texttt{\_top\_quantile\_p5\_p95}, \texttt{\_trimmed\_mean\_mad}) and operate directly on the empirical samples of each parameter, as produced by \texttt{GAWorker.finished}. Below, we describe each criterion in detail, including the meaning of all parameters and their statistical motivation.

\subsubsection{Interquartile Range (IQR, Q1--Q3)}
\vspace{-0.25em}
\begin{equation}
    L^{\text{IQR}}_j = Q_{x_j}(0.25), \qquad U^{\text{IQR}}_j = Q_{x_j}(0.75).
\end{equation}
Here, $Q_{x_j}(q)$ denotes the $q$-th quantile of the sample $\mathcal{X}_{p_j}$ for parameter $p_j$. The IQR interval captures the central 50\% of the observed values for $p_j$, i.e., the range between the first quartile (25th percentile) and the third quartile (75th percentile). This method is highly robust to outliers and is especially effective when the distribution of $p_j$ is unimodal or only mildly skewed. By focusing on the middle half of the data, it provides a compact summary of where most solutions concentrate, but it intentionally ignores the tails of the distribution.

\subsubsection{P5--P95 Quantile Interval}
\vspace{-0.25em}
\begin{equation}
    L^{\text{P5--P95}}_j = Q_{x_j}(0.05), \qquad U^{\text{P5--P95}}_j = Q_{x_j}(0.95),
\end{equation}
This interval is defined by the 5th and 95th percentiles of the sample for parameter $p_j$. It includes the central 90\% of the data, thus providing a broader coverage than the IQR. The P5--P95 interval is moderately robust to outliers: it trims the most extreme 5\% of values at each end, which is useful for distributions with moderately heavy tails. This criterion is often preferred when stakeholders desire a more generous envelope that still excludes the most extreme solutions.

\subsubsection{Tukey Whiskers (Classical Outlier Trimming)}
\begin{align}
    \left[ L^{\text{Tukey}}_j,\, U^{\text{Tukey}}_j \right] &= \left[
        \min\left\{ x \in \mathcal{X}_{p_j} : x \ge Q_1 - 1.5\,\mathrm{IQR} \right\},\;
        \max\left\{ x \in \mathcal{X}_{p_j} : x \le Q_3 + 1.5\,\mathrm{IQR} \right\}
    \right]
\end{align}
This approach trims classical outliers—values that fall outside the Tukey fences—while preserving the full variability of the interior data. It is especially interpretable and useful when a few extreme runs would otherwise distort the recommended range, but you still want to include as much of the “typical” data as possible.

\subsubsection{Shortest $\alpha$ High-Density Interval (HDI)}
The HDI criterion seeks the most compact interval containing a specified fraction $\alpha$ of the data. For parameter $p_j$, let $\alpha\in(0,1)$ (we use $\alpha=0.68$ by default, analogous to a 1-sigma interval for a normal distribution). Sort the sample values $v_1\le\dots\le v_n$ and set $k=\big\lfloor \alpha n\big\rceil$. The HDI is the shortest interval among all contiguous windows of length $k$:
\begin{equation}
    [L^{\text{HDI}}_j, U^{\text{HDI}}_j] = \operatorname*{arg\,min}_{1\le s\le n-k+1} (v_{s+k-1}-v_s).
\end{equation}
This method is non-parametric and adapts to the actual shape of the sample distribution, making it especially effective for skewed or multi-modal data. The HDI focuses on the densest region of solutions, which is often where the most promising or robust parameter values lie.

\subsubsection{Top-$q$ Performance P5--P95}
This criterion restricts attention to the best-performing fraction of runs, as measured by the fitness value $f^{(r)}$ for each run $r$. Let $q\in(0,1)$ denote the top fraction to consider (default $q=0.25$, i.e., the best 25\% of runs). Define the index set $\mathcal{I}_q = \{ r:\ f^{(r)} \leq Q_f(q) \}$, where $Q_f(q)$ is the $q$-th quantile of the fitness values. The subset of parameter samples from these top runs is $\mathcal{X}^{q}_{p_j}$. The interval is then:
\begin{equation}
    L^{\text{Top}}_j = Q_{x_j\in\mathcal{X}^{q}_{p_j}}(0.05),\qquad U^{\text{Top}}_j = Q_{x_j\in\mathcal{X}^{q}_{p_j}}(0.95).
\end{equation}
This approach highlights the range where high-performing solutions for $p_j$ are concentrated, making it particularly useful when the goal is to recommend parameter values that are most likely to yield elite performance. Note that this criterion is sensitive to the choice of $q$ and requires a sufficient number of top-performing samples for stability.

\subsubsection{Trimmed Mean $\pm$ 1.5\,MAD (TMAD, Robust)}
This robust criterion combines the trimmed mean and the median absolute deviation (MAD) to define a stable interval, even for small sample sizes or heavy-tailed distributions. For parameter $p_j$, let $m=\mathrm{median}(\mathcal{X}_{p_j})$ and $\mathrm{MAD}=\mathrm{median}(|x-m|)$, which measures the typical deviation from the median. The robust scale estimate is $\sigma_{\text{rob}}=1.4826\,\mathrm{MAD}$ (the constant makes it consistent with the standard deviation for normal data). Optionally, a 10\% symmetric trimming is applied before computing the mean $\tilde\mu$ (i.e., the mean of the central 80\% of values). The interval is:
\begin{equation}
    L^{\text{TMAD}}_j = \max\{\min\mathcal{X}_{p_j},\; \tilde\mu - 1.5\,\sigma_{\text{rob}}\},\quad
    U^{\text{TMAD}}_j = \min\{\max\mathcal{X}_{p_j},\; \tilde\mu + 1.5\,\sigma_{\text{rob}}\}.
\end{equation}
This method yields compact, outlier-resistant intervals and is especially reliable when the number of runs $R$ is small or the data are contaminated by outliers.

\subsection{Data ingestion and aggregation (GAWorker $\rightarrow$ GUI)}
\label{subsec:ingestion}
Each optimization run is executed by \texttt{GAWorker.run()}, which, upon completion, emits a set of outputs via \texttt{GAWorker.finished}:
\begin{itemize}
    \item \texttt{final\_results}: The full results of the run, optionally including \texttt{singular\_response} (detailed response data) and \texttt{benchmark\_metrics} (summary statistics and metadata).
    \item \texttt{best\_ind}: The best solution vector (i.e., the parameter values for the best individual found in the run).
    \item \texttt{parameter\_names}: An ordered list of parameter names, ensuring consistent mapping across runs.
    \item \texttt{best\_fitness}: The fitness value associated with the best solution.
\end{itemize}
The GUI mixin aggregates these outputs into a structured table, where each row corresponds to a run and columns include \texttt{run\_id}, \texttt{fitness}, and one column for each parameter (named according to \texttt{parameter\_names}). Additional metadata—such as the seeding method (\texttt{seeding\_method}), controller type (e.g., fixed, adaptive, ML-bandit, RL), and surrogate model settings—are attached from \texttt{benchmark\_metrics} to enable filtering and stratified analysis. Parameters that are fixed (i.e., have collapsed bounds and do not vary across runs) are automatically detected and treated as degenerate intervals (single values).

Special edge cases handled by the GUI include:
\begin{itemize}
    \item \textbf{Fixed parameters:} If a parameter is fixed across all runs, all criteria return the degenerate interval $[\theta^{\min},\theta^{\max}]$ for that parameter, reflecting its lack of variability.
    \item \textbf{Insufficient top-performing samples:} If the Top-$q$ subset contains fewer than 5 samples, the Top-$q$ P5--P95 interval is considered unstable and is omitted in favor of more robust criteria (HDI, IQR, TMAD).
\end{itemize}

\subsection{Criterion robustness and guidance}
Each interval criterion has distinct statistical properties and is suited to different data characteristics:
\begin{itemize}
    \item \textbf{IQR:} Highly robust to outliers and provides a compact interval. It is a good default when the parameter distribution is unimodal or only mildly skewed. By design, it ignores the tails, focusing on the central bulk of solutions.
    \item \textbf{P5--P95:} Offers broader coverage than IQR, including 90\% of the data. It is moderately robust and is appropriate when a more generous envelope is desired, while still trimming the most extreme values.
    \item \textbf{Tukey:} Implements classical outlier trimming using the 1.5 IQR rule. It is interpretable and effective when a few extreme runs would otherwise distort the range, but you want to retain as much of the “typical” data as possible.
    \item \textbf{HDI$_{0.68}$:} Finds the shortest interval containing 68\% of the data, regardless of the distribution’s shape. It excels for skewed or multi-modal samples and is especially useful for identifying dense “sweet spots” in the parameter space.
    \item \textbf{Top-$q$ P5--P95:} Focuses on the best-performing fraction of runs (e.g., top 25\% by fitness). This criterion is ideal when the recommended range should reflect where elite solutions are concentrated, but it is sensitive to the choice of $q$ and requires a sufficient number of top-performing samples.
    \item \textbf{TMAD:} Based on the median and MAD, with optional trimming, this is the most stable criterion for small sample sizes or when the data are heavy-tailed. It is conservative but highly reliable, making it a strong default in challenging scenarios.
\end{itemize}
\textbf{Recommended usage:} For most applications, begin with TMAD or IQR for stability and robustness. Add HDI to capture the densest region of solutions, especially if the distribution is skewed or multi-modal. Report P5--P95 for a broad, stakeholder-friendly envelope. Use Top-$q$ to highlight elite performance corridors, and apply Tukey when explicit, interpretable outlier handling is required.

\subsection{Algorithms}
Below we provide efficient, implementation-ready pseudocode for two of the more complex criteria, matching the helper functions in \texttt{ga\_mixin.py}. All variables are defined for clarity.

\begin{algorithm}[H]
\caption{Shortest $\alpha$-HDI for a parameter sample}
\begin{algorithmic}[1]
\REQUIRE Sample $\mathcal{X}=\{x_1,\dots,x_n\}$ for parameter $p_j$; desired coverage $\alpha\in(0,1)$ (default $0.68$)
\STATE $v \leftarrow \mathrm{sort}(\mathcal{X})$ ascending \hfill // Sort the sample values
\STATE $k \leftarrow \max\{1, \lfloor \alpha n \rceil\}$ \hfill // Number of points in the interval
\STATE $w^\star \leftarrow +\infty$, $s^\star \leftarrow 1$ \hfill // Initialize best width and start index
\FOR{$s=1$ to $n-k+1$}
    \STATE $w \leftarrow v_{s+k-1} - v_s$ \hfill // Width of current window
    \IF{$w < w^\star$} $w^\star\leftarrow w$, $s^\star\leftarrow s$ \ENDIF
\ENDFOR
\STATE \textbf{return} $[v_{s^\star},\; v_{s^\star+k-1}]$ \hfill // Shortest interval found
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{Top-$q$ performance P5--P95}
\begin{algorithmic}[1]
\REQUIRE Paired samples $\{(x^{(r)}, f^{(r)})\}_{r=1}^R$ for parameter $p_j$ and fitness; top fraction $q\in(0,1)$ (e.g., $q=0.25$)
\STATE $\tau \leftarrow Q_{\{f^{(r)}\}}(q)$ \hfill // Fitness threshold for top $q$ fraction
\STATE $\mathcal{J} \leftarrow \{ r:\ f^{(r)} \le \tau \}$ \hfill // Indices of top-performing runs
\STATE $\mathcal{X}^{q} \leftarrow \{ x^{(r)}:\ r\in\mathcal{J}\}$ \hfill // Parameter values from top runs
\STATE $L \leftarrow Q_{\mathcal{X}^{q}}(0.05)$, $U \leftarrow Q_{\mathcal{X}^{q}}(0.95)$ \hfill // 5th and 95th percentiles
\STATE \textbf{return} $[L, U]$
\end{algorithmic}
\end{algorithm}

\subsection{Aggregation, comparison, and decision rules}
Once all per-criterion intervals $[L^{(c)}_j, U^{(c)}_j]$ are computed for each parameter $p_j$, we aggregate and compare them using the following metrics and rules:
\begin{itemize}
    \item \textbf{Width and center:} For each criterion $c$, the width is $W^{(c)}_j=U^{(c)}_j-L^{(c)}_j$ and the center is $C^{(c)}_j=(U^{(c)}_j+L^{(c)}_j)/2$. These summarize the size and location of each interval.
    \item \textbf{Union band:} The union interval $[L^{\cup}_j, U^{\cup}_j]=[\min_c L^{(c)}_j,\; \max_c U^{(c)}_j]$ covers all values included by any criterion.
    \item \textbf{Intersection band:} The intersection $[L^{\cap}_j, U^{\cap}_j]=[\max_c L^{(c)}_j,\; \min_c U^{(c)}_j]$ is the region where all intervals overlap (if non-empty).
    \item \textbf{Majority intersection:} To avoid bias from a single criterion, we can intersect only those intervals supported by at least $M$ criteria (e.g., $M=3$ out of 6).
    \item \textbf{Consensus score:} $S^{\text{cons}}_j = \dfrac{\max\{0,\,U^{\cap}_j-L^{\cap}_j\}}{\max\{\varepsilon,\,U^{\cup}_j-L^{\cup}_j\}}$, where $\varepsilon\ll1$ prevents division by zero. This score quantifies the degree of agreement among criteria (1 = perfect consensus, 0 = no overlap).
    \item \textbf{Pairwise Intersection-over-Union (IoU):} For any two criteria $c_1$ and $c_2$, $\mathrm{IoU}^{(c_1,c_2)}_j = \dfrac{\max\{0,\,\min(U^{(c_1)}_j,U^{(c_2)}_j)-\max(L^{(c_1)}_j,L^{(c_2)}_j)\}}{\max\{\varepsilon,\,\max(U^{(c_1)}_j,U^{(c_2)}_j)-\min(L^{(c_1)}_j,L^{(c_2)}_j)\}}$ measures the overlap between intervals.
    \item \textbf{Normalized width:} $\tilde W^{(c)}_j = W^{(c)}_j/\max\{\varepsilon,\,U^{\cup}_j-L^{\cup}_j\}$ expresses each interval’s width as a fraction of the union width, allowing direct comparison of compactness across criteria.
\end{itemize}

\noindent\textbf{Recommended range selection.} The following decision rules are used to select the final recommended interval for each parameter $p_j$:
\begin{enumerate}[label=\textbf{R\arabic*}]
    \item If the intersection band $[L^{\cap}_j, U^{\cap}_j]$ is non-empty and its width $W^{\cap}_j$ exceeds a minimal engineering tolerance, use this as the recommended range.
    \item If the intersection is empty or too narrow, use the \emph{majority intersection} (intersection of at least $M$ criteria). If this is also empty, fall back to the shortest width $W^{(c)}_j$ among the robust criteria (HDI, IQR, TMAD).
    \item For parameters that show strong sensitivity to performance (e.g., high correlation with fitness), prefer the \emph{Top-$q$} or \emph{HDI} interval, as these focus on elite or dense regions.
    \item Always enforce physical or engineering bounds: clamp the final recommended interval $[L_j, U_j]$ to the allowed range $[\theta_j^{\min}, \theta_j^{\max}]$ for parameter $p_j$.
\end{enumerate}

\subsection{Diagnostics and visualization}
To support interpretation and transparency, we generate two main types of diagnostics for each parameter:
\begin{enumerate}
    \item \textbf{Stacked interval bands:} For each parameter, we plot stacked, color-coded rectangles representing the intervals from all criteria. This visualization makes it easy to see where criteria agree or disagree, and to compare the widths and locations of the intervals.
    \item \textbf{Width heatmap:} We construct a matrix where each row is a parameter and each column is a criterion, with entries showing the interval width $W^{(c)}_j$ (optionally normalized by the union width). This heatmap highlights which parameters are tightly constrained (stable) and which are more uncertain.
\end{enumerate}

\subsection{Statistical guidance on criterion choice}
The choice of interval criterion should be guided by the characteristics of the data and the goals of the analysis:
\begin{itemize}
    \item \textbf{Small sample size ($R$) or heavy outliers:} Prefer \emph{TMAD} and \emph{IQR}, as these are most robust and stable.
    \item \textbf{Skewed or multi-modal parameter distributions:} Use \emph{HDI}, which adapts to the actual density and can capture multiple modes or asymmetry.
    \item \textbf{When the recommended range should reflect high performance:} Use \emph{Top-$q$}, which focuses on the best-performing solutions.
    \item \textbf{When broader coverage is acceptable or desired:} Use \emph{P5--P95}, which includes most of the data while trimming only the extremes.
    \item \textbf{When explicit, interpretable outlier handling is needed:} Use \emph{Tukey}, which applies a classical, well-understood rule for outlier exclusion.
\end{itemize}

\subsection{Complexity and implementation notes}
The computational complexity of the statistical range synthesis procedure is dominated by the sorting operations required for quantile-based criteria. For each parameter, most criteria (such as IQR, P5--P95, Tukey, and HDI) require sorting the $R$ values collected from independent optimization runs, resulting in a per-parameter complexity of $O(R \log R)$. Simple quantile calculations (e.g., computing the median or a single percentile) can be performed in $O(R)$ time using selection algorithms, but in practice, sorting is used for flexibility and simplicity, especially when multiple quantiles or more complex intervals (like HDI or Tukey) are needed.

Since the procedure is repeated for each of the $P$ parameters, the total computational cost scales as $O(P R \log R)$. This linear scaling in the number of parameters $P$ and near-linear scaling in the number of runs $R$ makes the method practical and efficient even for high-dimensional problems or when aggregating results from hundreds of optimization runs. 

In the software implementation, the GUI leverages efficient, vectorized operations provided by NumPy and Pandas. These libraries allow for rapid computation of quantiles, sorting, and interval construction across all parameters and runs. The mapping from the mathematical definitions of each criterion to code is direct: for example, the IQR is computed using \texttt{numpy.percentile} or \texttt{pandas.Series.quantile}, and the HDI is implemented via a sliding window over the sorted samples. This ensures that the statistical analysis remains both robust and performant, supporting interactive exploration and visualization in the GUI.

\subsection{End-to-end procedure (from optimization to ranges and comparison)}
The following algorithm outlines the complete workflow, starting from the outputs of repeated optimization runs and culminating in the generation, comparison, and export of recommended parameter ranges. Each step is designed to ensure traceability, robustness, and transparency in the statistical synthesis process.

\begin{algorithm}[H]
\caption{Aggregate and Validate Optimization Run Data}
\begin{algorithmic}[1]
\REQUIRE $R$ independent optimization runs executed via \texttt{GAWorker.run()}; each run outputs a tuple \texttt{(final\_results, best\_ind, parameter\_names, best\_fitness)}; parameter bounds $[\theta_j^{\min},\theta_j^{\max}]$ for all $j$; optional metadata (e.g., seeding method, controller type, surrogate usage)
\STATE \textbf{Data collection:} In the GUI, aggregate all runs into a structured table. Each row corresponds to a run and contains the run identifier (\texttt{run\_id}), the final fitness value (\texttt{fitness}), and one column for each parameter in \texttt{parameter\_names[j]}.
\STATE \textbf{Validation and alignment:} Ensure that parameter names and ordering are consistent across all runs. Remove any runs that are incomplete, invalid, or contain missing data to maintain data integrity.
\STATE \textbf{Sample construction:} For each parameter $p_j$, extract the set of observed values across all runs, forming the sample $\mathcal{X}_{p_j} = \{x^{(r)}_j\}_{r=1}^R$. Simultaneously, collect the corresponding fitness values $\{f^{(r)}\}_{r=1}^R$. Identify parameters that are fixed (i.e., have the same value in all runs).
\STATE \textbf{Top-$q$ subset (optional):} If desired, define a subset of runs corresponding to the top-performing fraction $q$ (e.g., $q=0.25$). Compute the fitness threshold $\tau = Q_f(q)$, and let $\mathcal{I}_q = \{ r : f^{(r)} \leq \tau \}$ be the indices of the top runs. For each parameter, extract the corresponding values $\mathcal{X}^{q}_{p_j}$.
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{Compute Statistical Intervals for Each Parameter}
\begin{algorithmic}[1]
\REQUIRE For each parameter $p_j$, sample $\mathcal{X}_{p_j}$ (and optionally $\mathcal{X}^{q}_{p_j}$), parameter bounds $[\theta_j^{\min}, \theta_j^{\max}]$
\FOR{$j=1$ to $P$}
    \IF{$p_j$ is fixed}
        \STATE For all criteria, set the interval to the full allowed range $[\theta_j^{\min}, \theta_j^{\max}]$ (since no variability is observed), and \textbf{continue} to the next parameter.
    \ENDIF
    \STATE \textbf{Interval computation:} For parameter $p_j$, compute the recommended intervals according to each statistical criterion using GUI helper functions:
        \begin{itemize}
            \item \textbf{IQR:} Interquartile range (25th to 75th percentile)
            \item \textbf{P5--P95:} 5th to 95th percentile interval
            \item \textbf{Tukey:} Tukey's outlier rule (using $1.5\times$ IQR)
            \item \textbf{HDI$_{0.68}$:} Highest Density Interval containing 68\% of the data
            \item \textbf{Top-$q$ P5--P95:} 5th to 95th percentile within the top-$q$ subset (if $|\mathcal{I}_q|$ is sufficiently large)
            \item \textbf{TMAD:} Trimmed Median Absolute Deviation interval
        \end{itemize}
\ENDFOR
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{Compute Interval Metrics and Compare Criteria}
\begin{algorithmic}[1]
\REQUIRE For each parameter $p_j$, intervals $[L^{(c)}_j, U^{(c)}_j]$ for all criteria $c$
\FOR{$j=1$ to $P$}
    \STATE For each criterion $c$, calculate the interval width $W^{(c)}_j = U^{(c)}_j - L^{(c)}_j$ and center $C^{(c)}_j = (U^{(c)}_j + L^{(c)}_j)/2$.
    \STATE Compute the union interval $[L^{\cup}_j, U^{\cup}_j]$ (spanning all criteria) and the intersection interval $[L^{\cap}_j, U^{\cap}_j]$ (where all intervals overlap).
    \STATE Quantify agreement and differences among criteria by computing:
        \begin{itemize}
            \item \textbf{Consensus score} $S^{\text{cons}}_j$ (fraction of overlap among all intervals)
            \item \textbf{Pairwise Intersection-over-Union (IoU)} between all pairs of criteria
            \item \textbf{Normalized widths} $\tilde W^{(c)}_j$ (width of each interval relative to the union width)
        \end{itemize}
\ENDFOR
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{Select Recommended Parameter Ranges}
\begin{algorithmic}[1]
\REQUIRE For each parameter $p_j$, all computed intervals, metrics, and parameter bounds $[\theta_j^{\min}, \theta_j^{\max}]$
\FOR{$j=1$ to $P$}
    \STATE Apply the decision rules (R1--R4) to select the final recommended interval $[L_j, U_j]$ for parameter $p_j$. This may involve using the intersection, majority intersection, or the most robust criterion, and always clamping to the physical bounds $[\theta_j^{\min}, \theta_j^{\max}]$.
\ENDFOR
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{Visualize, Report, and Export Results}
\begin{algorithmic}[1]
\REQUIRE For all parameters, per-criterion intervals, metrics, and final recommended ranges
\STATE Present the results in the GUI as follows:
    \begin{itemize}
        \item Per-criterion tables showing $[L^{(c)}_j, U^{(c)}_j, W^{(c)}_j, C^{(c)}_j]$ for all parameters
        \item Stacked-interval plots visualizing the intervals for each parameter and criterion
        \item Overlays comparing intervals across criteria, width heatmaps, and summary metrics
    \end{itemize}
\STATE Export all computed intervals, union/intersection bands, comparison metrics, and final recommended ranges $[L_j, U_j]$ to CSV files. These are saved alongside the \texttt{benchmark\_metrics} for full traceability and reproducibility of the analysis.
\end{algorithmic}
\end{algorithm}

\subsection{Mapping to software implementation}
The GUI adds a \emph{Parameter Ranges} benchmarking tab with one subtab per criterion and a \emph{Compare Criteria} subtab. Each subtab presents: (i) a table of $[L^{(c)}_j, U^{(c)}_j, W^{(c)}_j, C^{(c)}_j]$ across all parameters; (ii) a horizontal stacked-rectangle visualization; and (iii) export options (CSV). The comparison subtab overlays stacked rectangles across selected criteria and provides a combined table to quantitatively compare ranges.

\vspace{0.5em}
\noindent\textbf{Engineering outcome.} The final ranges provide defensible, transparent envelopes for each DVA parameter, reflecting both central tendency and high-performance concentration while remaining robust to outliers and distributional irregularities.



\end{document} 